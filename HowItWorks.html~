<!DOCTYPE html>
<html>
<head>
<script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [["$","$"],["\\(","\\)"]]
      }
      });
    </script>
    <script type="text/javascript"
	    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
<meta name="format-detection" content="telephone=no">


<title>OACC - Online Algorithmic Complexity Calculator</title>
<style type="text/css">

.default {
	font-family: Verdana, Geneva, sans-serif;
}

.default {
	font-family: "Courier New", Courier, monospace;
}

.default {
	font-family: "Lucida Sans Unicode", "Lucida Grande", sans-serif;
}

a:link {
	color: #F60;
	text-decoration: none;
}

a:visited {
	color: #F60;
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:active {
	text-decoration: none;
}

.center-justified {
  text-align: justify;
  margin: 0 auto;
  width: 55em;
}


</style>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>

  <body background="./images/TuringField.png" class="default">
  <center>
   <a href="index.html">Main Page</a> | <a href="HowItWorks.html">How It Works</a> | <a href="HowToCite.html">How To Cite</a> | <a href="team.html">Team</a> | <a href="https://github.com/andandandand/OACC">GitHub Repo</a> | <a href="DownloadDataAndTools.html">Download Data and Tools</a> | <a href="http://algorithmicnature.wordpress.com/publications/">Publications</a>
  <h1><a href="index.html">The Online Algorithmic Complexity Calculator</a></h1>
  </center>

<center>

  <div class="center-justified">
  <h2>Overview</h2>
  <p>This introductory video provides a brief overview for non-experts in the area explaining how both the Coding Theorem Method ($CTM$) and the Block Decomposition Method ($BDM$) approximate universal measures of algorithmic complexity.</p>

<center>
  <iframe seamless src="https://www.youtube.com/embed/BEaXyDS_1Cw" style="border: none; width: 50vw; height: 30vw" allowfullscreen></iframe>

  <div class="center-justified">
      <p align="justify">The&nbsp;<strong>Online Algorithmic Complexity Calculator (OACC)</strong>&nbsp;is an online&nbsp;tool developed&nbsp;by the&nbsp;<a href="http://algorithmicnature.org/" target="_blank">Algorithmic Nature Group</a>&nbsp;to provide reliable estimations&nbsp;to non-computable functions. This is achieved&nbsp;through various numerical methods based upon the mathematical theory of&nbsp;algorithmic probability and&nbsp;algorithmic randomness. The estimations have a wide range of applications&nbsp;in a   range of disciplines&nbsp;from molecular biology, to cognitive science,&nbsp;time series research (e.g. finance), and graph theory (for a list of examples, see <a href="http://algorithmicnature.wordpress.com/publications/">Publications</a>).</p>
</div>

<div class="center-justified">
<p>The <strong>OACC</strong> provides numerical approximations (upper bounds) of <a href="http://www.scholarpedia.org/article/Algorithmic_complexity">Algorithmic (Kolmogorov-Chaitin) Complexity</a> (AC) for short strings ($\textit{CTM}$), for strings of any length ($\textit{BDM}_{1D}$), and for binary arrays ($\textit{BDM}_{2D}$), which can represent the adjacency matrices of unweighted graphs. These techniques are not only an alternative to the widespread use of lossless compression algorithms to approximate AC, but true approaches to AC. Lossless compression algorithms (e.g. BZIP2, GZIP, LZ) that are based upon entropy rate are not more related to AC than Shannon Entropy itself, which is unable to compress anything but statistical regularities.</p>
</div>
</center>
    <h2>Advantage of CTM/BDM over Entropy and Lossless Compression</h2>
    
	<p>The following plots show quantifications of the power of $CTM$ and $BDM$ to identify objects that look statistically random (high Entropy) but are actually causal in nature, as they are produced by short computer programs and thus recursively produced by a generation mechanism that Entropy (and lossless compression algorithms based on Entropy rate) overlooks.</p>
    <center>
	  <p><img src="./images/causalgaps.png" width="716" height="528" alt=""/></p>
    </center>
	<div class="center-justified">
  <p><strong>Plot A: </strong>X axis: Plot of values for a random sample of binary strings of length 100 bits sorted by increasing number of non-zeros. Y axis: Normalized values between 0 and 1 for each of the 3 measures ($BDM$, Compress and Entropy) used to approximate algorithmic complexity $K$. The lossless compression curve (here the algorithm used is Compress, but similar behaviour for LZ, LZW, etc) closely follow the  classical Bernoulli distribution for the Entropy of the set of strings. This does not come by surprise because implementations of lossless compression algorithms are actually simply Entropy-rate estimators (up to a fixed window length size). However, the $BDM$ distribution approximates the expected theoretical semi-exponential distribution for $K$  assigning lower values to strings that are causally generated but are statistically random-looking to Entropy and compression.</br>
  <strong>Plot B: </strong>Same test with bitstrings of length 1000 ($BDM$ starts approximating Shannon Entropy if $CTM$ is not upated).</br>
  <strong>Plot C: </strong>$BDM$ provides a much finer-grained distribution where Shannon Entropy and lossless compression algorithms retrieve only a limited number (about 5) of differentiated 'complexity' values for the all strings of length 12.</br>
  <strong>Plot D: </strong>Number of strings (of length 12) among all $2^{12}$ bitstrings with maximum Shannon Entropy or near maximum Shannon Entropy but low algorithmic complexity estimated by $CTM$/$BDM$. Arrows poiting to what we call 'causal gaps' show the power gained by using $CTM$/$BDM$ against Shannon Entropy and compression at identifying causally generated strings and objects that may not have any statistical patterns but can be recursively/algorithmically produced with a short computer program. More information on $CTM$ is available <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0096223">in this paper</a> and on Entropy and $BDM$ <a href="https://arxiv.org/abs/1609.00110">in this other one</a>.</p></div>
	<p>&nbsp;</p>
  </div>
</center>

<div class="center-justified">
  <h2>Choosing evaluation parameters for  Block Decomposition </h2>

    <p>If the string that you want to evaluate has length shorter than $13$, you should use the Coding Theorem Method ($\textit{CTM}$) to estimate its algorithmic complexity. Otherwise, you should use the Block Decomposition Method ($\textit{BDM}$), which requires specifying block size and block overlap values if you don't want to use the (optimal) values. The key is to always compare among values with same block size and block overlap values, and not among different ones (as the size and overlap may under- or over-estimate  complexity)</p>

<p>The following is a $BDM$ partitioning example for block size $= 6$ and block overlap $= 1$ as an illustration of the meaning of block size and block overlap in the estimation of a complexity of a long string: </p>

<p align="center"><img src="./images/examplediagram.png" width="432" height="139" alt=""/></p>


<p>
$BDM$ is defined by $$\textit{BDM} = \sum_{i}^{n} \textit{CTM}(\textit{block}_i) + \log_{2}(|\textit{block}_i|),$$ where $\textit{CTM}(\textit{block}_i)$ denotes the approximated algorithmic complexity of the pattern in the block of symbols, and $|\textit{block}_i|$ denotes the number of occurrences (multiplicity) of the block.
</p>

    <p>For $\textit{BDM}$, optimal parameters are usually the largest possible block size ($= 12$) with no overlaping ($= 0$). You should always pick the largest available block size, as it provides better approximations to algorithmic complexity. In contrast, the smallest block size ($=1$) approximates Shannon Entropy. You can pick any overlapping value that is shorter than your block size. For example, say your string is $111001010111$ and you use block size $=6$. If the overlapping is $0$, then $\textit{BDM}$ will look up the known $\textit{CTM}$ values of $111001$ and $010111$ and add them up, outputting $29.9515$ bits. Alternatively, you can choose block overlap $=1$, for which the strings whose $\textit{CTM}$ values are added are $111001$, $101011$, and $010111$. This second evaluation will output $32.9672$ bits.</p>
  </div>
  <div class="center-justified">
    <p>Overlapping helps to deal with leftovers of the block partitioning if the string length is not a multiple of the block size, otherwise, leftovers with length less than the block size will be discarded and won't be considered in the complexity estimation. <a href="https://arxiv.org/abs/1609.00110">This paper</a> shows different schemes to deal with boundary conditions. The online calculator only deals with the most 'basic' of these schemes, but we have proven that the error is bounded, and thus the output values are reliable for most comparative purposes. In general, overlapping blocks produce overestimations of complexity, and non-overlapping blocks lead to an underestimation only for objects with dimensions that are not a multiple of the block size. </p>
    <p> You should always compare results with the same chosen parameters (unless you estimate the error as we did in <a href="https://arxiv.org/abs/1609.00110">this paper</a> and then make corrections or take the deviations into consideration).</p>
  </div>
  <div class="center-justified"><p>As for matrices, the same rule holds. Current support for strings is binary and non-binary, but for arrays it's currently only binary. With some loss of precision, one can always translate any alphabet into binary with some loss of information due to the extra granularity introduced in the translation.<br></p></div>

<div class="center-justified">
  <h2>Estimating Algorithmic Probability (AP)</h2>

<p>For more technical details you must read the papers listed in the Bibliography Section below. In a nutshell, we calculate a function $D(n,m)(s)$, which estimates the <strong><em>Algorithmic Probability</em></strong> of a string $s$ from a set of halting Turing machines with $n$ states and $m$ symbols denoted by $(n,m)$. We use the standard model of Turing machines used by Tibor Rado in the definition of the <a href="https://youtu.be/CE8UhcyJS0I">Busy Beaver problem</a>, but we have also proven that radical changes to the model produce similar estimations. Beyond the known values of the Busy Beaver problem we have also shown that  educated choices of reasonable halting times can be chosen to reach certainty up to any arbitrary statistical significance level.</p>
<p>Formally,
  $$D(n, m)(s)=\frac{|\{T\in(n, m) : T(p) = s,\ b\in\{0,1,\cdots,m-1\}\}|}{|\{(T,b)\in(n, m)\times \{0,1,\cdots,m-1\} : T(b) \textit{ halts }\}|},$$
  where $T(b)$ represents the output of Turing machine $T$ when running on a blank tape filled with symbol $b$ that produces $s$ upon halting, and $|A|$ represents the cardinality of the set $A$. </p>

<p>For $(n,2)$ with $ n < 5 $, the known Busy Beaver values give the maximum number of steps that a machine can run upon halting. But for $n \geq 5$ or for $(n,m)$ with $m > 2$, no Busy Beaver values are known, and the size of the machine spaces make impossible a complete exploration to calculate $D(n,m)$ for arbitrary $n$ and $m$, but an educated choice of timeouts can be made and samples produced (see  Bibliography section below).</div><p>

<div class="center-justified">
<h2>Approximating Algorithmic Complexity (K) by CTM and BDM</h2>

<p>The function $D(n,m)$ is an approximation to Levin's Universal Distribution $\mathfrak{m}(s)$, and it can be used to approximate $K(s)$, the algorithmic complexity of string $s$ by using the Coding Theorem,
  $$K(s) \simeq -\log_2 \mathfrak{m}(s)$$</p>
<p>
The greater value of $n$ we use to calculate $D(n,m)$, the better approximations we make to $K(s)$ for $s$ a string of an alphabet of $m$ symbols. Due to the uncomputability of $D(n,m)$ we work with samples and runtime cutoffs. For the simulation of Turing machines we use a C++ simulator running on a supercomputer of medium size.
<p>$BDM$ extends the power of $CTM$ as explained in <a href="https://arxiv.org/abs/1609.00110">this paper</a>.
<p>
</div>
<div class="center-justified">
  <div class="center-justified">
    <h2>Approximating Bennett's Logical Depth by CTM and BDM</h2>
    <p>The $CTM$ allows us not only to build an empirical distribution of computer program outputs from smallest to larger size but once a string is generated for the first time among all the computer programs of the smallest size, we know which Turing machine is the smallest one producing a string and also the runtime of such a Turing machine. We take the runtime of such a Turing machine as an estimation of Bennett's Logical Depth ($LD$) by $CTM$, and also extend the power of $CTM$ to estimate $LD$ with a multiplicative variation of the $BDM$ formula. Despite the fact that $LD$ is neither lower nor upper semi-computable (and therefore truly non-computable), estimations by $CTM$ and $BDM$ do produce the  characteristic concave distribution assigning algorithmic random strings lower logical depth, thereby conforming with the theoretical expectation unlike Shannon Entropy and lossless compression:</p>
    <center>
    <p><img src="images/LogicalDepth.png" width="641" height="683"  alt=""/></p>
    <div class="center-justified"><p>Unlike approximations to algorithmic complexity by lossless compression (top left plot), $LD$-based values using $CTM$ and $BDM$ conform to theoretical expectation regarding $LD$ behaviour. The behaviour is confirmed among all the $BDM$ variations according to different <a href="https://arxiv.org/abs/1609.00110">boundary conditions schemes</a>, and it could not be reproduced either using Shannon Entropy (or Entropy rate) nor using lossless compression algorithms. Further information is available in <a href="https://arxiv.org/abs/1609.00110">this paper</a>.</p></div>
    <a name="personalrandomness"></a>
    </center>
    <h2>Calculating your ability to behave random by producing a random-looking sequence
      and grid</h2>
    <p>In an <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005408" target="new">article</a> published by PLoS Computational Biology we have shown that the ability to produce algorithmic randomness peaks at age 25. The article was <a href="http://www.hectorzenil.net/media.html" target="new">widely covered by the world media</a>.  You can test your own personal ability using this calculator. The results in the paper are given in what are called 'z-scores', to obtain yours and be able to compare your results to those in the paper, you  only need  to apply the following  formula to the result you obtain from this calculator: your <strong>z-score = $(K-m)/s$</strong>, where $K$ is the result you will obtain from this calculator for a sequence (tab 'For short strings' use $CTM$ value) or for a grid (tab 'For binary arrays' $BDM$ value). The values for $m$ and $s$, and the parameters to choose in the calculator are in the following table:
    <table border="1" align="center" summary="Means and standard deviations">
      <caption>
        Your z-score calculation
      </caption>
      <tr>
        <th scope="col">item</th>
        <th scope="col"><blockquote>task</blockquote></th>
        <th scope="col"><blockquote>
          <p>m</p>
        </blockquote></th>
        <th scope="col"><blockquote>
          <p>s</p>
        </blockquote></th>
        <th scope="col"><blockquote>
          <p>alphabet</p>
        </blockquote></th>
        <th scope="col">overlap</th>
        <th scope="col"><blockquote>
          <p>length</p>
        </blockquote></th>
      </tr>
      <tr>
        <th scope="row">1</th>
        <th scope="row"><blockquote>
          <p>Toss</p>
        </blockquote></th>
        <td><blockquote>
          <p>32.5</p>
        </blockquote></td>
        <td><blockquote>
          <p>1.56</p>
        </blockquote></td>
        <td><blockquote>
          <blockquote>
            <p>2</p>
          </blockquote>
        </blockquote></td>
        <td><blockquote>
          <p>0</p>
        </blockquote></td>
        <td><blockquote>
          <p> 12</p>
        </blockquote></td>
      </tr>
      <tr>
        <th scope="row">2</th>
        <p>Guess</p>
        <td><blockquote>
          <p>34.7</p>
        </blockquote></td>
        <td><blockquote>
          <p>1.07</p>
        </blockquote></td>
        <td><blockquote>
          <blockquote>
            <p>5</p>
          </blockquote>
        </blockquote></td>
        <td><blockquote>
          <p>0</p>
        </blockquote></td>
        <td><blockquote>
          <p>10</p>
        </blockquote></td>
      </tr>
      <tr>
        <th scope="row">3</th>
        <th scope="row">Spot</th>
        <td><blockquote>
          <p>41.41</p>
        </blockquote></td>
        <td><blockquote>
          <p>0.93</p>
        </blockquote></td>
        <td><blockquote>
          <blockquote>
            <p>9</p>
          </blockquote>
        </blockquote></td>
        <td><blockquote>
          <p>0</p>
        </blockquote></td>
        <td><blockquote>
          <p>10</p>
        </blockquote></td>
      </tr>
      <tr>
        <th scope="row">4</th>
        <th scope="row">Roll</th>
        <td><blockquote>
          <p>36.62</p>
        </blockquote></td>
        <td><blockquote>
          <p>1.04</p>
        </blockquote></td>
        <td><blockquote>
          <blockquote>
            <p>6</p>
          </blockquote>
        </blockquote></td>
        <td><blockquote>
          <p>0</p>
        </blockquote></td>
        <td><blockquote>
          <p>10</p>
        </blockquote></td>
      </tr>
      <tr>
        <th scope="row">5</th>
        <th scope="row"><blockquote>
          <p>Grid</p>
        </blockquote></th>
        <td><blockquote>
          <p>17.01</p>
        </blockquote></td>
        <td><blockquote>
          <p>1.08</p>
        </blockquote></td>
        <td><blockquote>
          <blockquote>
            <p>2D</p>
          </blockquote>
        </blockquote></td>
        <td><blockquote>
          <p>0</p>
        </blockquote></td>
        <td><blockquote>
          <p>3x3</p>
        </blockquote></td>
      </tr>
    </table>
    <blockquote>
      <blockquote>&nbsp;</blockquote>
    </blockquote>
  </div>
  <h2>Version history and future of the OACC</h2>
  <p>We keep developing the calculator towards wider horizons of methodological and numerical capabilities:</p>
  <ul>
    <li><strong>Version 1</strong>: Estimations of $K$ for short binary strings</li>
    <li><strong>Version 2</strong>:  $CTM$ and $BDM$ expanded capabilities for non-binary strings and  arrays</li>
    <li><strong>Version 2.5</strong> (<em><strong>current version</strong></em>): Estimations of (Bennett's) logical depth based on $CTM$</li>
    <li><strong>Version 3</strong>: Algorithmic-information dynamics of strings, arrays and networks</li>
    <li><strong>Version 4</strong>: Algorithmic complexity of models, algorithmic feature selection, algorithmic dimensionality reduction and model generator.</li>
  </ul>
  <p>  
</div>
<p>&nbsp;</p>
<div class="center-justified">
  <h2><strong>Bibliography</strong></h2>

<p> Delahaye J.-P. and Zenil, H. (2012) Numerical Evaluation of the Complexity of Short Strings: A Glance Into the Innermost Structure of Algorithmic Randomness.
  <a href="http://www.sciencedirect.com/science/article/pii/S0096300311012367">Applied Mathematics and Computation</a> 219, pp. 63-77. </br>
  <a href="BibTex/numericalBibTex.txt" target="_blank">BibTex entry</a></p>

<p>Soler-Toscano F., Zenil H., Delahaye J.-P. and Gauvrit N. (2014) Calculating Kolmogorov Complexity from the Output Frequency Distributions of Small Turing Machines. <a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0096223">PLoS ONE 9(5): e96223.</a></br>
        <a href="BibTex/frequencyBibTex.txt" target="_blank">BibTex entry</a>

<p>Zenil H., Soler-Toscano F., Dingle K. and Louis A. (2014) Correlation of Automorphism Group Size and Topological Properties with Program-size Complexity Evaluations of Graphs and Complex Networks, <a href="http://www.sciencedirect.com/science/article/pii/S0378437114001691">Physica A: Statistical Mechanics and its Applications</a>, vol. 404, pp. 341–358, 2014.</br>
<a href="BibTex/graphBibTex.txt" target="_blank">BibTex entry</a></p>
<p>Zenil H., Soler-Toscano F., Kiani N.A.,  Hernández-Orozco S.,  Rueda-Toicen A. (2016) A Decomposition Method for Global Evaluation of Shannon Entropy and Local Estimations of Algorithmic Complexity, <a href="https://arxiv.org/abs/1609.00110">arXiv:1609.00110</a></a></br>
        <a href="BibTex/bdmBibTex.txt" target="_blank">BibTex entry</a></p>
<p>See the <a href="http://algorithmicnature.wordpress.com/publications/">Publications</a> section for even more details and  applications  to other areas and connections to other measures.</p><p>&nbsp;</p></div>
<table width="797" border="0" align="center">
  <tr>
    <td width="302"><h6>Content on<span href="http://creativecommons.org" property="cc:attributionName" rel="cc:attributionURL"> this site</span> is licensed under a<br />
      <a rel="license" href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 License</a><br />
      <br />
      <a href="http://creativecommons.org/licenses/by/3.0"><img src="./images/CC.png" alt="Creative Commons Licence Attribution 3.0 Unported (CC BY 3.0)" width="88" height="31" border="0" longdesc="http://creativecommons.org/licenses/by/3.0/" /><br />
      </a><a href="http://creativecommons.org/licenses/by/3.0">View License Deed</a> | <a href="http://creativecommons.org/licenses/by/3.0/legalcode">View Legal Code</a></h6></td>
    <td width="485"><h6>Contact info: hector.zenil at algorithmicnaturelag dot org</h6>
      <h6>If you use results from the OACC in a publication, please visit <a href="HowToCite.html">How to Cite.</a><br />
         <br>
      <a href="http://www.algorithmicnature.net">Algorithmic Nature Group</a> - <a href="http://www.labores.eu">LABORES</a></h6></td>
  </tr>
</table>
<br>
</center>

</body>
</html>
